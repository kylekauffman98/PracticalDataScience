{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will explain and illustrate some of the key differences between two data processing libraries, Pandas and PySpark. While both can be used to similar effects, PySpark has several unique features that separate it from Pandas, namely its ability to run operations in parallel and in distributed enviornments. A Pandas Dataframe is constrained in size by the memory of the server the application is executed on, which can limit the amount of data processed. A PySpark dataframe on the other hand can also act as a distributed SQL like query engine, enabling large amounts of data to be stored and read from a single dataframe, even in a distributed environment. This is especially useful for Big-Data applications. While many of its benefits are focused on distributed computing applications and Big-Data, this tutorial will provide some basics of using the PySpark library, including reading in data, creating user-defined functions, and using the PySpark machine learning modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will show how to set up PySpark in a Jupyter Notebook and how using PySpark can enable big data processing.\n",
    "\n",
    "We will be using Amazon product data gathered from UCSD's Julian McAuley in order to execute operations. While PySpark is best suited to distributed execution, this tutorial will focus on execution on a single node. However, these operations are ubiquitous to both local and distributed Spark use cases. \n",
    "\n",
    "- [Installing Libraries](#section1)\n",
    "- [Installing Java, Spark, etc](#section2)\n",
    "- [SparkSession vs SparkContext](#section3)\n",
    "- [Reading in Data with PySpark](#section4)\n",
    "- [Reading in Data with Pandas](#section5)\n",
    "- [Predicting Amazon Ratings from Reviews on Big Data Sets](#section6)\n",
    "- [Summary and References](#section7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "# Installing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, you will need to install PySpark using pip:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip3 install --upgrade pyspark`\n",
    "or\n",
    "`conda install -c conda-forge pyspark`\n",
    "additionally, if any error with python version mismatch occurs when executing any of the code, you may need to upgrade your python version to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pandas as pd\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, CountVectorizer, StopWordsRemover, IDF\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as T\n",
    "import re\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since pyspark relies on Apache Spark, we need to install Apache Spark on our system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "# Installing Java, Spark (and winutils for windows)\n",
    "The following instructions are for Windows only. See [Note](#Note) for Mac."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Java\n",
    "In addition to pyspark, we will need to install Java 8 or newer. https://www.java.com/en/download/. To check if already installed, type `java -version` in your command prompt. This tutorial walks through how to install with windows. The process may differ slightly for MacOS or other operating systems.\n",
    "\n",
    "### Spark\n",
    "Go to https://spark.apache.org/downloads.html and download the latest version of Apache Spark. Install directly under your C: directory in a file path with no spaces. We will refer to this spark download folder as SPARK_HOME. After downloading, navigate to bin\\pyspark.exe and execute. There should be numerous error messages, with one stating that the winutils binary could not be located. \n",
    "\n",
    "### Winutils\n",
    "Navigate to https://github.com/steveloughran/winutils and download the bin of the corresponding hadoop version you selected when downloading Spark. Move the bin to a new folder called `hadoop` within the SPARK_HOME folder. In order for these effects to take place, we must set the SPARK_HOME and HADOOP_HOME environment variables. Type in `enviornment variables` in the windows search bar and select the tile `Edit the system environment variables`. Click the Enviornment_Variables button at the bottom of the advanced tab. Under system variables, click New... Create a SPARK_HOME enivornment variable with the correct path and a HADOOP_HOME enivornment variable with a path to the hadoop folder.\n",
    "\n",
    "Rerun pyspark.exe. There may still be an error stating `Unable to load native-hadoop library for your platform... using builtin-java classes where applicable` however this occurs if you are running on a 64-bit machine and the hadoop libraries are compiled for 32-bits. This warning can be ignored.\n",
    "\n",
    "### Fixing log level for Spark\n",
    "1) Save a copy of the log4j.properties.template file in the \\conf folder under SPARK_HOME and rename it to log4j.propertiesfile.\n",
    "\n",
    "2) Set the log4j.rootCategory value in the file to WARN, console and save.\n",
    "\n",
    "Now rerunning pyspark should only display warnings and errors to the console.\n",
    "<a id='Note'></a>\n",
    "### Note: For MacOS\n",
    "To install apache spark, you only need to install with homebrew: `homebrew install apache-spark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "# Spark Session vs Spark Context\n",
    "Prior to the 2.0 version of Spark, a Spark Context was considered the entry point to the spark application and the entry point was created using `sc = SparkContext(\"local\", \"SparkFile App\")`. For other applications, including SQL, SQL Context, or streaming, a specific context had to be created. Now, a spark session creates a new spark context for all of the above and is contained within the SparkSession. To use the spark context, create the spark session and access the spark context with .sparkContext(). An example is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session\n",
    "sc = SparkSession.builder.appName(\"AmazonData\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Dataframe Structure\n",
    "PySpark Dataframes are based on the PySpark Resilient Distributed Dataset (RDD). This is a fault-tolerant collection of elements that can be operated on in parallel. RDDs can be created using the parallelize() function from existing data, or read in from files. In the example below, we will create an RDD from a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an RDD using the spark context of 1 million numbers from 0 on.\n",
    "rdd1 = sc.sparkContext.parallelize(range(100000))\n",
    "# Get the number of partitions.\n",
    "rdd1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the number of partitions depends on the number of cores in your CPU. In this example, the computer this code was executed on has 6 physical cores and 12 logical cores, therefore there are 12 Partitions. Each partition is a separate portion of the data that can be processed in parallel. Next we will define some function which we will apply to the RDD and compare to the execution time to the same function applied to a python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an arbitrary function that compute the tangent of a number and stores in a list 10,000 times.\n",
    "# This is simply meant to be computationally intensive.\n",
    "from math import tan\n",
    "def time_function(x):\n",
    "    t = [tan(j) for j in range(10000)]\n",
    "    return tan(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-3.380515006246586"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "time_function(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "map_test = rdd1.map(lambda x: time_function(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the operation executes instantly. This is because PySpark uses lazy evaluation, meaning nothing is actually computed, only a plan of what to execute. The execution only takes place once the data is actually accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50005\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Count the number of entries where the values is greater than 0.\n",
    "print(map_test.filter(lambda x:x>0).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50005\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Perform the equivalent operations on a python list. All of the setup is included, since this is all included in the \n",
    "# computation time above.\n",
    "large_list = [i for i in range(100000)]\n",
    "large_list = list(map(lambda x: time_function(x), large_list))\n",
    "print(len(list(filter(lambda x: x>0, large_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaulating Performance\n",
    "Notice that even though the same operations are taking place, the execution time of the PySpark RDD is significantly faster. Though with smaller list sizes python will win out because of the overhead of parallelism, when larger data is used, the parallel computation of Spark means it will take less time. This execution time is only further lowered when used in a distributed enviornment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "# Reading in Data with PySpark\n",
    "Since we are reading data from a URL, we are going to use SparkFiles to download the file to the spark context and then read the file into a spark dataframe. While not necessary for this problem, we will extract the name of the csv from the url and the name of the json file, since spark uses an absolute file context and the name is required to access the files from the context. In this example, we are loading 1,297,156 ratings and 151,254 reviews.\n",
    "\n",
    "Check out http://jmcauley.ucsd.edu/data/amazon/ for a list of all available datasets. The data we are using is only a small subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file. This may take some time to download.\n",
    "url1 = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Grocery_and_Gourmet_Food.csv\"\n",
    "sc.sparkContext.addFile(url1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract filename\n",
    "filename1 = url1.split(r'/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a schema. This csv does not include column names, therefore we must define them. We do not want to infer the \n",
    "# schema, therefore we will use pyspark types to define the types.\n",
    "schema = T.StructType([\\\n",
    "    T.StructField(\"user\", T.StringType(), True),\\\n",
    "    T.StructField(\"item\", T.StringType(), True),\\\n",
    "    T.StructField(\"rating\", T.StringType(), True),\\\n",
    "    T.StructField(\"timestamp\", T.IntegerType(), True)])\n",
    "# Read the file from the Spark context. No header is present in the csv, therefore header=False.\n",
    "df_spark_csv = sc.read.csv(SparkFiles.get(filename1), header=False, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the dataframe\n",
    "In order to get an idea of what the PySpark dataframe looks like, we use the .show() command, similar to .head(n) in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+------+----------+\n",
      "|          user|      item|rating| timestamp|\n",
      "+--------------+----------+------+----------+\n",
      "|A1ZQZ8RJS1XVTX|0657745316|   5.0|1381449600|\n",
      "|A31W38VGZAUUM4|0700026444|   5.0|1354752000|\n",
      "|A3I0AV0UJX5OH0|1403796890|   1.0|1385942400|\n",
      "|A3QAAOLIXKV383|1403796890|   3.0|1307836800|\n",
      "| AB1A5EGHHVA9M|141278509X|   5.0|1332547200|\n",
      "+--------------+----------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_csv.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "# Reading in Data from Pandas\n",
    "\n",
    "One benefit to using PySpark to read in data from a url is that the file is saved in the spark context and can be accessed easily after the download. Reading a url directly into a pandas dataframe requires the file to be downloaded each time, unless another library is used to donwload the url, like the requests library. Though this is fairly simple, it can be an extra required step. However, a benefit to pandas is that the types are inferred easily and a struct type schema does not need to be defined in order to name the columns like in spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice this must be run each time you want to read in the file, unless you save the csv.\n",
    "df_pandas_csv = pd.read_csv(url1, names=['user', 'item', 'rating', 'timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the dataframe\n",
    "In order to get an idea of what the Pandas dataframe looks like, we use the .head() command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1ZQZ8RJS1XVTX</td>\n",
       "      <td>0657745316</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1381449600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A31W38VGZAUUM4</td>\n",
       "      <td>0700026444</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1354752000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3I0AV0UJX5OH0</td>\n",
       "      <td>1403796890</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1385942400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3QAAOLIXKV383</td>\n",
       "      <td>1403796890</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1307836800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AB1A5EGHHVA9M</td>\n",
       "      <td>141278509X</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1332547200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user        item  rating   timestamp\n",
       "0  A1ZQZ8RJS1XVTX  0657745316     5.0  1381449600\n",
       "1  A31W38VGZAUUM4  0700026444     5.0  1354752000\n",
       "2  A3I0AV0UJX5OH0  1403796890     1.0  1385942400\n",
       "3  A3QAAOLIXKV383  1403796890     3.0  1307836800\n",
       "4   AB1A5EGHHVA9M  141278509X     5.0  1332547200"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas_csv.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "# Predicting Amazon Ratings from Reviews on Big Data Sets\n",
    "What makes PySpark unique is its inclusion of its Spark Machine Learning Library, which is similar to Scikit-Learn. In this example, we will show how to predict a user's rating from the review text using PySpark's machine learning library. We will use Logistic Regression to predict and classify the associated review text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Grocery_and_Gourmet_Food_5.json.gz\"\n",
    "sc.sparkContext.addFile(url)\n",
    "# extract filename\n",
    "filename_json_reviews = url.split(r'/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can just as easily read compressed, json files, using .read.json()\n",
    "df_json_reviews = sc.read.json(SparkFiles.get(filename_json_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the data\n",
    "Let's get a better idea of what data we are working with and which columns are important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+--------------------+-----------+--------------+---------------+--------------------+--------------+\n",
      "|      asin|helpful|overall|          reviewText| reviewTime|    reviewerID|   reviewerName|             summary|unixReviewTime|\n",
      "+----------+-------+-------+--------------------+-----------+--------------+---------------+--------------------+--------------+\n",
      "|616719923X| [0, 0]|    4.0|Just another flav...| 06 1, 2013|A1VEELTKS8NLZB|Amazon Customer|          Good Taste|    1370044800|\n",
      "|616719923X| [0, 1]|    3.0|I bought this on ...|05 19, 2014|A14R9XMZVJ6INB|        amf0001|3.5 stars,  sadly...|    1400457600|\n",
      "|616719923X| [3, 4]|    4.0|Really good. Grea...| 10 8, 2013|A27IQHDZFQFNGG|        Caitlin|                Yum!|    1381190400|\n",
      "|616719923X| [0, 0]|    5.0|I had never had i...|05 20, 2013|A31QY5TASILE89|   DebraDownSth|Unexpected flavor...|    1369008000|\n",
      "|616719923X| [1, 2]|    4.0|I've been looking...|05 26, 2013|A2LWK003FFMCI5|       Diana X.|Not a very strong...|    1369526400|\n",
      "+----------+-------+-------+--------------------+-----------+--------------+---------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json_reviews.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing unwanted columns\n",
    "Similar to pandas, PySpark dataframes can be manipulated using SQL like operations. In this case we will just select the overall and reveiwText columns to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = [\"overall\",  \"reviewText\"]\n",
    "# Select returns a new PySpark Dataframe\n",
    "df_json_reviews = df_json_reviews.select([column for column in keep_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now df_json_reviews just has overall and reviewText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|overall|          reviewText|\n",
      "+-------+--------------------+\n",
      "|    4.0|Just another flav...|\n",
      "|    3.0|I bought this on ...|\n",
      "|    4.0|Really good. Grea...|\n",
      "|    5.0|I had never had i...|\n",
      "|    4.0|I've been looking...|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json_reviews.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the Model Pipeline\n",
    "The pipeline will be broken down into several steps listed below:\n",
    "\n",
    "1) Remove any html tags and then tokenize\n",
    "\n",
    "2) Remove stopwords using the built in StopWordsRemoval and the default stop words\n",
    "\n",
    "3) Compute HashingTF which maps a sequence of terms to their term frequencies using a hashing trick (or optionally Count Vectorizer)\n",
    "\n",
    "4) Compute TF-IDF from HashingTF (or Count Vectorizer)\n",
    "\n",
    "5) Logistic Regression\n",
    "\n",
    "6) Use PySpark ml pipeline to execute the above stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Dataframes are immutable, meaning preprocessing the strings is not exactly the same as pandas. Here, we are going to create a user defined function (UDF) to remove urls and perform other processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a UDF?\n",
    "In order to preprocess the data, we must use a user-defined function. A user-defined function is used to manipulate data in a PySpark Dataframe. PySpark dataframes are immutable, so any operation performed on the dataframe creates a new dataframe. Here we use a UDF to add a new column which is a filtered version of the text, removing links, apostrophes, and spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(text):\n",
    "    text = text.lower()\n",
    "    # remove http links\n",
    "    text = re.sub(r'http[s]?:\\/\\/t.co\\/[\\w]+', '', text)\n",
    "    # remove trailing 's chars followed by other apostrophes\n",
    "    text = re.sub(r\"'s\", '', text)\n",
    "    text = re.sub(r\"'\", '', text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", ' ', text)\n",
    "    return text\n",
    "replaceUDF = F.udf(lambda z: replace(z), T.StringType())\n",
    "df_cleaned = df_json_reviews.withColumn(\"cleanedText\", replaceUDF(F.col(\"reviewText\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|overall|          reviewText|         cleanedText|\n",
      "+-------+--------------------+--------------------+\n",
      "|    4.0|Just another flav...|just another flav...|\n",
      "|    3.0|I bought this on ...|i bought this on ...|\n",
      "|    4.0|Really good. Grea...|really good great...|\n",
      "|    5.0|I had never had i...|i had never had i...|\n",
      "|    4.0|I've been looking...|ive been looking ...|\n",
      "|    4.0|These Kit-kats ar...|these kit kats ar...|\n",
      "|    3.0|I found these in ...|i found these in ...|\n",
      "|    5.0|Creamy white choc...|creamy white choc...|\n",
      "|    5.0|After hearing mix...|after hearing mix...|\n",
      "|    1.0|I love green tea,...|i love green tea ...|\n",
      "|    5.0|I ordered these i...|i ordered these i...|\n",
      "|    5.0|These are definit...|these are definit...|\n",
      "|    5.0|Yes - this is one...|yes this is one o...|\n",
      "|    5.0|I love the green ...|i love the green ...|\n",
      "|    3.0|I love Kit Kat & ...|i love kit kat gr...|\n",
      "|    4.0|I tried this for ...|i tried this for ...|\n",
      "|    5.0|This curry paste ...|this curry paste ...|\n",
      "|    5.0|I've purchased di...|ive purchased dif...|\n",
      "|    5.0|I love ethnic foo...|i love ethnic foo...|\n",
      "|    4.0|I started a new d...|i started a new d...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data into Training and Test\n",
    "PySpark also offers an easy way to split data into training and testing. By using the .randomSplit() method, we can randomly separate the data into training and test dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70% training, 30% testing.\n",
    "trainingData, testData = df_cleaned.randomSplit([0.7, 0.3], seed=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we added another column called cleanedText which applies some preprocessing, including removing urls, apostrophes and non alpha numeric characters.\n",
    "\n",
    "### Tokenization\n",
    "Now let's tokenize the text and output it to a new column called \"words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"cleanedText\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Removal\n",
    "Next we use the StopWordsRemover to output the text with common stop words removed. In order to see these words, we can run .getStopWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"StopRemoved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now',\n",
       " \"i'll\",\n",
       " \"you'll\",\n",
       " \"he'll\",\n",
       " \"she'll\",\n",
       " \"we'll\",\n",
       " \"they'll\",\n",
       " \"i'd\",\n",
       " \"you'd\",\n",
       " \"he'd\",\n",
       " \"she'd\",\n",
       " \"we'd\",\n",
       " \"they'd\",\n",
       " \"i'm\",\n",
       " \"you're\",\n",
       " \"he's\",\n",
       " \"she's\",\n",
       " \"it's\",\n",
       " \"we're\",\n",
       " \"they're\",\n",
       " \"i've\",\n",
       " \"we've\",\n",
       " \"you've\",\n",
       " \"they've\",\n",
       " \"isn't\",\n",
       " \"aren't\",\n",
       " \"wasn't\",\n",
       " \"weren't\",\n",
       " \"haven't\",\n",
       " \"hasn't\",\n",
       " \"hadn't\",\n",
       " \"don't\",\n",
       " \"doesn't\",\n",
       " \"didn't\",\n",
       " \"won't\",\n",
       " \"wouldn't\",\n",
       " \"shan't\",\n",
       " \"shouldn't\",\n",
       " \"mustn't\",\n",
       " \"can't\",\n",
       " \"couldn't\",\n",
       " 'cannot',\n",
       " 'could',\n",
       " \"here's\",\n",
       " \"how's\",\n",
       " \"let's\",\n",
       " 'ought',\n",
       " \"that's\",\n",
       " \"there's\",\n",
       " \"what's\",\n",
       " \"when's\",\n",
       " \"where's\",\n",
       " \"who's\",\n",
       " \"why's\",\n",
       " 'would']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words.getStopWords() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Term Frequency\n",
    "Now let's compute the term frequency and output it to a column called \"features\". This can be done using CountVectorizer or HashingTF. Feel free to try it both ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = HashingTF(inputCol=stop_words.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "#tf = CountVectorizer(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "idf = IDF(inputCol=tf.getOutputCol(), outputCol=\"features\", minDocFreq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Now let's set up logistic regression. Feel free to experiment wiht the maxIter. The regularization parameters are tested below during K-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10000).setLabelCol(\"overall\").setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the built in ml Pipeline\n",
    "PySpark's ml library allows us to set up steps in our pipeline and adjust parameters before executing. This enables us to string together multiple steps and then create a pipeline to fit the training data to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, stop_words, tf, idf, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built in K-fold cross validation\n",
    "Another unique feature of PySpark is the ability to perform cross validation with ease, simply by forming a parameter grid and using the CrossValidator constructor. You can specify the estimator, the parameters, the evaluator, and the number of folds. In this case, we use our MulticlassClassification Evaluator with accuracy as the metric and set the label, prediction, and features columns to the correct names in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(tf.numFeatures, [1000, 5000, 10000]) \\\n",
    "    .addGrid(lr.regParam, [1, 0.5, 0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"overall\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
    "                          numFolds=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this next cell will take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = crossval.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cvModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.selectExpr(\"overall as label\", \"prediction\", \"cleanedText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  1.0|       5.0|\n",
      "|  1.0|       5.0|\n",
      "|  1.0|       4.0|\n",
      "|  1.0|       5.0|\n",
      "|  1.0|       5.0|\n",
      "+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"label\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model Accuracy\n",
    "In the following cells, we use our MulticlassClassificationEvaluator to determine the accuracy of the model we constructed above. Keep in mind that we are trying to classify to 5 different categories simply based on text input. We can also set the metric name to other metrics. By default, the metric is F1-score. Here we use accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6425628483927116\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "print(\"Accuracy: \" + str(evaluator.setMetricName(\"accuracy\").evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretting the Result\n",
    "While determining the actual star rating is interesting, we will also calculate the average difference between the actual rating and the predicted ratings to evaluate what this accuracy actually means. Ultimately, we have an average difference of -0.447 and a standard deviation of 0.99. While these results are not outstanding, this example shows the ability to implement a multiclass classifier easily in PySpark, using built in machine learning functions. Keep in mind that we are not truly interpreting this result. To do so, we would need to look at a confusion matrix, which can be implemented, but strangely is difficult to use on a single machine since one of the workers fails after running out of memory. In reality, there are far more 5 star reviews, so by predicting 5 stars more frequently, accuracy may be higher, but once again, this tutorial is just a baseline for using the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|diff|\n",
      "+----+\n",
      "|-4.0|\n",
      "|-4.0|\n",
      "|-3.0|\n",
      "|-4.0|\n",
      "|-4.0|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sim = predictions.withColumn(\"diff\", F.col(\"label\") -F.col(\"prediction\"))\n",
    "sim.select(\"diff\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          avg(diff)|\n",
      "+-------------------+\n",
      "|-0.4469341441381893|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sim.agg(F.mean('diff')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "| stddev_samp(diff)|\n",
      "+------------------+\n",
      "|0.9936002514173153|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sim.agg(F.stddev('diff')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "# Summary and References\n",
    "Ultimately this tutorial highlighted some key features of PySpark and how Spark may be useful for data processing applications. PySpark's all-in-one like approach can make developing machine learning applications more scalable, especially for larger datasets. \n",
    "\n",
    "By initially developing in a local enivornment, bugs can be worked out before deploying to a web-service cluster, like on Amazon web services or google cloud. While Spark's potential is limited in this local setting, understanding some of its fundamentals is essential to a successful cluster deployment.\n",
    "\n",
    "1. PySpark Documentation : https://spark.apache.org/docs/latest/api/python/index.html\n",
    "2. Amazon data: http://jmcauley.ucsd.edu/data/amazon/\n",
    "3. Spark Download: https://spark.apache.org/downloads.html\n",
    "4. Winutils: https://github.com/steveloughran/winutils"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
